{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Learning Assignment - Image Caption Generator Model"
      ],
      "metadata": {
        "id": "D-Wskt0lPBxi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction"
      ],
      "metadata": {
        "id": "5w-9a_XRPHHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Team members"
      ],
      "metadata": {
        "id": "9FwwmKPoPKHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Mahavithana S. G - 2020/E/087\n",
        "* Weerakoon A. B   - 2020/E/169\n",
        "* Somapala M. S    - 2020/E/193"
      ],
      "metadata": {
        "id": "p56PO3y7PMBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "t_O9AMcmPPac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "G14Nlmf0PQqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "\n",
        "import IPython\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "xTlKCIHtPULF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data loading and pre-processing"
      ],
      "metadata": {
        "id": "NEKkjxnwPTx1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UASynvSPGsS",
        "outputId": "151ade95-adbe-414e-c7f3-c8c5ea9983dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting data directories"
      ],
      "metadata": {
        "id": "pJiDY9_CPpmd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bpQBZNC3OV2o"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/drive/My Drive/Image caption generator Dataset\"\n",
        "image_dir = data_dir + \"/Images\"\n",
        "csv_file = data_dir + \"/captions.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading captions"
      ],
      "metadata": {
        "id": "AildxUdzPzuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caption_df = pd.read_csv(csv_file, delimiter=',')\n",
        "\n",
        "print(f'The shape of dataframe: {caption_df.shape}')\n",
        "print(f'The columns in the dataframe: {caption_df.columns}')\n",
        "print('First 10 rows of the dataframe:')\n",
        "print(caption_df.head(10))\n",
        "print(f'Unique image names: {len(pd.unique(caption_df[\"image\"]))}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7vB6SSqP16H",
        "outputId": "4ee2a555-710c-4703-f6c3-6734550574dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of dataframe: (40455, 2)\n",
            "The columns in the dataframe: Index(['image', 'caption'], dtype='object')\n",
            "First 10 rows of the dataframe:\n",
            "                       image  \\\n",
            "0  1000268201_693b08cb0e.jpg   \n",
            "1  1000268201_693b08cb0e.jpg   \n",
            "2  1000268201_693b08cb0e.jpg   \n",
            "3  1000268201_693b08cb0e.jpg   \n",
            "4  1000268201_693b08cb0e.jpg   \n",
            "5  1001773457_577c3a7d70.jpg   \n",
            "6  1001773457_577c3a7d70.jpg   \n",
            "7  1001773457_577c3a7d70.jpg   \n",
            "8  1001773457_577c3a7d70.jpg   \n",
            "9  1001773457_577c3a7d70.jpg   \n",
            "\n",
            "                                             caption  \n",
            "0  A child in a pink dress is climbing up a set o...  \n",
            "1              A girl going into a wooden building .  \n",
            "2   A little girl climbing into a wooden playhouse .  \n",
            "3  A little girl climbing the stairs to her playh...  \n",
            "4  A little girl in a pink dress going into a woo...  \n",
            "5         A black dog and a spotted dog are fighting  \n",
            "6  A black dog and a tri-colored dog playing with...  \n",
            "7  A black dog and a white dog with brown spots a...  \n",
            "8  Two dogs of different breeds looking at each o...  \n",
            "9    Two dogs on pavement moving toward each other .  \n",
            "Unique image names: 8091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffle dataframe"
      ],
      "metadata": {
        "id": "M3albFykQFoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caption_df = caption_df.sample(frac=1).reset_index(drop=True)\n",
        "print(caption_df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi8bL1lZQFNU",
        "outputId": "7c2aa6e7-6f5e-4f19-f90e-38aee9aaf448"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       image  \\\n",
            "0  2704362232_7d84503433.jpg   \n",
            "1  3545427060_c16a8b7dfd.jpg   \n",
            "2  3424605029_53078d3505.jpg   \n",
            "3   330849796_c575c3108a.jpg   \n",
            "4  3530087422_7eb2b2c289.jpg   \n",
            "5  2968135512_51fbb56e3e.jpg   \n",
            "6  3249062399_0dafe5e4f5.jpg   \n",
            "7  3437273677_47d4462974.jpg   \n",
            "8  2696060728_3043cfc38c.jpg   \n",
            "9  2881468095_d4ce8c0c52.jpg   \n",
            "\n",
            "                                             caption  \n",
            "0   A man and dog are splashing in a swimming pool .  \n",
            "1             A group of kids play around in water .  \n",
            "2  A young boy wearing a blue shirt is playing at...  \n",
            "3  A woman , curled up , sleeps in her seat on a ...  \n",
            "4  A person in a red and white suit kneels down o...  \n",
            "5     Young skateboarder doing a trick in the park .  \n",
            "6       Three women walk in a line on the sidewalk .  \n",
            "7  A woman eating at a restaurant with other peop...  \n",
            "8  A man helps a girl get up on a turquoise surfb...  \n",
            "9      A skateboarder is balancing on a brick wall .  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean captions and start/end tags"
      ],
      "metadata": {
        "id": "NMrVIzAUQQFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_captions(captions):\n",
        "\n",
        "    # remove punctuations\n",
        "    captions = re.sub(r'[^\\w\\s]', '', captions)\n",
        "\n",
        "    # convert to lowercase\n",
        "    captions = captions.lower()\n",
        "\n",
        "    # remove multiple consecutive spaces\n",
        "    captions = re.sub(r'\\s+', ' ', captions).strip()\n",
        "    return captions\n",
        "\n",
        "caption_df['caption'] = caption_df['caption'].apply(clean_captions)\n",
        "\n",
        "# add start and end to captions\n",
        "caption_df['caption'] = '<start> ' + caption_df['caption'] + ' <end>'\n",
        "\n",
        "print('First 10 rows of the dataframe cleaned:')\n",
        "print(caption_df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hH2vNEsQQba",
        "outputId": "afe36349-d3da-49de-d872-84ff080cbf46"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 rows of the dataframe cleaned:\n",
            "                       image  \\\n",
            "0  2704362232_7d84503433.jpg   \n",
            "1  3545427060_c16a8b7dfd.jpg   \n",
            "2  3424605029_53078d3505.jpg   \n",
            "3   330849796_c575c3108a.jpg   \n",
            "4  3530087422_7eb2b2c289.jpg   \n",
            "5  2968135512_51fbb56e3e.jpg   \n",
            "6  3249062399_0dafe5e4f5.jpg   \n",
            "7  3437273677_47d4462974.jpg   \n",
            "8  2696060728_3043cfc38c.jpg   \n",
            "9  2881468095_d4ce8c0c52.jpg   \n",
            "\n",
            "                                             caption  \n",
            "0  <start> a man and dog are splashing in a swimm...  \n",
            "1  <start> a group of kids play around in water <...  \n",
            "2  <start> a young boy wearing a blue shirt is pl...  \n",
            "3  <start> a woman curled up sleeps in her seat o...  \n",
            "4  <start> a person in a red and white suit kneel...  \n",
            "5  <start> young skateboarder doing a trick in th...  \n",
            "6  <start> three women walk in a line on the side...  \n",
            "7  <start> a woman eating at a restaurant with ot...  \n",
            "8  <start> a man helps a girl get up on a turquoi...  \n",
            "9  <start> a skateboarder is balancing on a brick...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set absolute image paths"
      ],
      "metadata": {
        "id": "B0vrYpdoQlIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caption_df['image'] = image_dir + '/' +caption_df['image']"
      ],
      "metadata": {
        "id": "MsTleliDQpw_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split training, validating and testing datasets"
      ],
      "metadata": {
        "id": "naE7o4CkRRn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.7 * len(caption_df))\n",
        "val_size = int(0.2 * len(caption_df))\n",
        "\n",
        "# split datasets\n",
        "train_df = caption_df[:train_size]\n",
        "val_df = caption_df[train_size:train_size+val_size]\n",
        "test_df = caption_df[train_size+val_size:]"
      ],
      "metadata": {
        "id": "e7PKS22-RRYc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup tokenizer"
      ],
      "metadata": {
        "id": "5KbIJVp3SC01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pick 5000 most recurring words\n",
        "VOCAB_SIZE = 5000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<unk>\", filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~')\n",
        "\n",
        "# fit tokenizer on captions\n",
        "tokenizer.fit_on_texts(caption_df['caption'])\n",
        "\n",
        "# check and verify the tokenizer\n",
        "print(f'Vocabulary size: {len(tokenizer.word_index) + 1}')\n",
        "dog_idx = tokenizer.word_index['dog']\n",
        "print(f'Index of dog: {dog_idx}')\n",
        "print(f'Word with index {dog_idx}: {tokenizer.index_word[dog_idx]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8-0IiBYSE-1",
        "outputId": "4fe0095e-c75a-476b-eab0-964b2de56018"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 8832\n",
            "Index of dog: 10\n",
            "Word with index 10: dog\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess and generate caption datasets"
      ],
      "metadata": {
        "id": "O_C1ZX_8UAFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tensorflow_caption_dataset_from_dataframe(dataframe):\n",
        "  sequences = tokenizer.texts_to_sequences(dataframe['caption'])\n",
        "  padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
        "  return tf.data.Dataset.from_tensor_slices(padded_sequences)\n",
        "\n",
        "train_cap_ds = tensorflow_caption_dataset_from_dataframe(train_df)\n",
        "val_cap_ds = tensorflow_caption_dataset_from_dataframe(val_df)\n",
        "test_cap_ds = tensorflow_caption_dataset_from_dataframe(test_df)"
      ],
      "metadata": {
        "id": "ieDCN_XfT_oq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading images"
      ],
      "metadata": {
        "id": "eJ4_CAjZVDrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def load_img(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    img = tf.image.resize(img, (224, 224))\n",
        "    return img"
      ],
      "metadata": {
        "id": "6EjXrxkjVDW0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup image datasets"
      ],
      "metadata": {
        "id": "5aTpZA4YVU2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tensorflow_image_dataset_from_dataframe(dataframe):\n",
        "  image_paths = dataframe['image'].values\n",
        "  return tf.data.Dataset.from_tensor_slices(image_paths).map(load_img)\n",
        "\n",
        "train_img_ds = tensorflow_image_dataset_from_dataframe(train_df)\n",
        "val_img_ds = tensorflow_image_dataset_from_dataframe(val_df)\n",
        "test_img_ds = tensorflow_image_dataset_from_dataframe(test_df)"
      ],
      "metadata": {
        "id": "9UxyFZS0VbhW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Definition\n",
        "\n",
        "The **show and tell** model contains two main components,\n",
        "\n",
        "<ol>\n",
        "  <li> Show (Encoder)\n",
        "    <ul>\n",
        "      <li>Encompases image feature extraction</li>\n",
        "      <li>Uses CNN from InceptionV3</li>\n",
        "      <li>Passes extracted features as the initial hidden state</li>\n",
        "    </ul>\n",
        "  </li>\n",
        "  <li> Tell (Decoder)\n",
        "    <ul>\n",
        "      <li>Encompases caption generation</li>\n",
        "      <li>Accepts the initial hidden state</li>\n",
        "      <li>Uses LSTMs (or GRUs) to generate token indieces which can be mapped to words from the loaded vocab</li>\n",
        "    </ul>\n",
        "  </li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "HWlUfQWEZufJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder"
      ],
      "metadata": {
        "id": "EK7Ao3DNaWA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, activation='sigmoid'):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.activation = activation\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.resnet = tf.keras.applications.ResNet50(include_top=False, weights='imagenet')\n",
        "        self.resnet.trainable = False\n",
        "        self.gap = tf.keras.layers.GlobalAveragePooling2D()\n",
        "        self.fc = tf.keras.layers.Dense(units=self.embedding_dim, activation=self.activation)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.resnet(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "6a8-56UHaVn_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder"
      ],
      "metadata": {
        "id": "Q751k2VwaYV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units, vocab_size, rnn_type='GRU', num_layers=4):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.units = units\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.rnn_type = rnn_type\n",
        "        self.num_layers = num_layers\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim)\n",
        "\n",
        "        self.rnn_layers = []\n",
        "        for _ in range(self.num_layers):\n",
        "            if self.rnn_type == 'GRU':\n",
        "                self.rnn_layers.append(tf.keras.layers.GRU(units=self.units, return_sequences=True, return_state=True))\n",
        "            elif self.rnn_type == 'LSTM':\n",
        "                self.rnn_layers.append(tf.keras.layers.LSTM(units=self.units, return_sequences=True, return_state=True))\n",
        "\n",
        "        self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "        self.fc2 = tf.keras.layers.Dense(self.vocab_size)\n",
        "\n",
        "    def call(self, x, intialize_to_zero=False):\n",
        "        initial_state = self.reset_state(batch_size=x.shape[0])\n",
        "\n",
        "        output = x\n",
        "        state = None\n",
        "\n",
        "        for rnn_layer in self.rnn_layers:\n",
        "          if intialize_to_zero:\n",
        "            output_tuple = rnn_layer(inputs=output, initial_state=initial_state)\n",
        "            output = output_tuple[0]\n",
        "            state = output_tuple[1]\n",
        "          else:\n",
        "            output_tuple = rnn_layer(inputs=output)\n",
        "            output = output_tuple[0]\n",
        "            state = output_tuple[1]\n",
        "\n",
        "        x = self.fc1(output)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x, state\n",
        "\n",
        "    def embed(self, x):\n",
        "        return self.embedding(x)\n",
        "\n",
        "    def reset_state(self, batch_size):\n",
        "        if self.rnn_type == 'GRU':\n",
        "            return tf.zeros((batch_size, self.units))\n",
        "        elif self.rnn_type == 'LSTM':\n",
        "            return [tf.zeros((batch_size, self.units)), tf.zeros((batch_size, self.units))]"
      ],
      "metadata": {
        "id": "zfgDQt4oaYF6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define loss function"
      ],
      "metadata": {
        "id": "GnYvlC3zdm9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(real, pred, loss_object):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "qV5UAMJ_dmln"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define train function"
      ],
      "metadata": {
        "id": "926i5LgNdy4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target, optimizer, encoder, decoder, loss_object):\n",
        "    loss = 0\n",
        "\n",
        "    # Record the operations for automatic differentiation\n",
        "    with tf.GradientTape() as tape:\n",
        "        features = tf.expand_dims(encoder(img_tensor), 1)\n",
        "        em_words = decoder.embed(target)\n",
        "        x = tf.concat([features, em_words], axis=1)\n",
        "        predictions, _ = decoder(x, True)\n",
        "\n",
        "        # Compute the loss between the target and predictions\n",
        "        loss = loss_function(target[:, 1:], predictions[:, 1:-1, :], loss_object)\n",
        "\n",
        "    # Get the trainable variables from both the encoder and decoder\n",
        "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    # Compute the gradients of the loss with respect to the trainable variables\n",
        "    gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "    # Apply the gradients to update the trainable variables using the optimizer\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Lq1yDucWZuIq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define validation function"
      ],
      "metadata": {
        "id": "JIWfj0VJfCdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def val_step(img_tensor, target, encoder, decoder, loss_object):\n",
        "    loss = 0\n",
        "    features = tf.expand_dims(encoder(img_tensor),1)\n",
        "    em_words = decoder.embed(target)\n",
        "    x = tf.concat([features,em_words],axis=1)\n",
        "    predictions, _ = decoder(x, True)\n",
        "    loss = loss_function(target[:,1:], predictions[:,1:-1,:], loss_object)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "GkpvTRfZZM-q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train and return model"
      ],
      "metadata": {
        "id": "DiHWqvaofTLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_return_model(train_img_ds,\n",
        "                           train_cap_ds,\n",
        "                           val_img_ds,\n",
        "                           val_cap_ds,\n",
        "                           epochs=15,\n",
        "                           batch_size=512,\n",
        "                           embedding_dim=512,\n",
        "                           decoder_dense_units=256,\n",
        "                           vocab_size=VOCAB_SIZE,\n",
        "                           rnn_type='GRU',\n",
        "                           num_layers=4):\n",
        "  # zip and prefetch datasets\n",
        "  train_ds = tf.data.Dataset.zip((train_img_ds, train_cap_ds)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  val_ds = tf.data.Dataset.zip((val_img_ds, val_cap_ds)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  # get encoder and decoder\n",
        "  encoder = Encoder(embedding_dim=embedding_dim)\n",
        "  decoder = Decoder(embedding_dim=embedding_dim, units=decoder_dense_units, vocab_size=vocab_size, rnn_type=rnn_type, num_layers=num_layers)\n",
        "\n",
        "  # setup optimizer and loss object\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "  # train the model for number of epochs\n",
        "  epoch_wise_loss = []\n",
        "  epoch_wise_val_loss = []\n",
        "  for epoch in tqdm(range(epochs), desc=\"Epochs: \"):\n",
        "    batch_wise_loss = []\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(tqdm(train_ds, desc=\"Training Batches: \", leave=False)):\n",
        "        loss = train_step(img_tensor, target, optimizer, encoder, decoder, loss_object)\n",
        "        batch_wise_loss.append(loss.numpy())\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            print(f'Epoch: {epoch} Batch: {batch} Loss: {batch_wise_loss[-1]:.3f}')\n",
        "\n",
        "    epoch_wise_loss.append(np.mean(batch_wise_loss))\n",
        "\n",
        "    batch_wise_val_loss = []\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(tqdm(val_ds, desc=\"Validation Batches: \", leave=False)):\n",
        "        loss = val_step(img_tensor, target, encoder, decoder, loss_object)\n",
        "        batch_wise_val_loss.append(loss.numpy())\n",
        "\n",
        "    epoch_wise_val_loss.append(np.mean(batch_wise_val_loss))\n",
        "\n",
        "    print(f'Epoch: {epoch} Total Loss: {epoch_wise_loss[-1]:.3f} Val Loss: {epoch_wise_val_loss[-1]:.3f}')\n",
        "    print('-' * 40)\n",
        "\n",
        "  return encoder, decoder, epoch_wise_loss, epoch_wise_val_loss\n"
      ],
      "metadata": {
        "id": "x3mkIy6Cfenx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test train function"
      ],
      "metadata": {
        "id": "5yakZpuRlV5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder, decoder, epoch_wise_loss, epoch_wise_val_loss = train_and_return_model(train_img_ds,\n",
        "#                                                                                 train_cap_ds,\n",
        "#                                                                                 val_img_ds,\n",
        "#                                                                                 val_cap_ds)"
      ],
      "metadata": {
        "id": "GxEuleFklWNX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install and login to wandb"
      ],
      "metadata": {
        "id": "ELzed-qUA1TR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU"
      ],
      "metadata": {
        "id": "CLlwcajDA9Eq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3183c05-1d30-4cc3-efd6-fa016fd94838"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "_Sz6eusjBBDS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "29a57506-e203-4f75-ea62-2df807d81155"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run pre-configured sweep"
      ],
      "metadata": {
        "id": "dEGI7M8hA8mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tune():\n",
        "  # initialize a new run\n",
        "  wandb.init()\n",
        "\n",
        "  # define hyperparameters to tune\n",
        "  config = wandb.config\n",
        "\n",
        "  encoder, decoder, epoch_wise_loss, epoch_wise_val_loss = train_and_return_model(train_img_ds,\n",
        "                                                                                  train_cap_ds,\n",
        "                                                                                  val_img_ds,\n",
        "                                                                                  val_cap_ds,\n",
        "                                                                                  epochs=config.epochs,\n",
        "                                                                                  batch_size=config.batch_size,\n",
        "                                                                                  embedding_dim=config.embedding_dim,\n",
        "                                                                                  decoder_dense_units=config.decoder_dense_units,\n",
        "                                                                                  rnn_type=config.rnn_type,\n",
        "                                                                                  num_layers=config.num_layers)\n",
        "\n",
        "  wandb.log({\"val_loss\": np.mean(np.array(epoch_wise_val_loss)), \"epoch_wise_val_loss\": epoch_wise_val_loss})\n",
        "\n",
        "sweep_id = \"40z0uu0q\"\n",
        "\n",
        "wandb.agent(sweep_id, function=tune)"
      ],
      "metadata": {
        "id": "MqSGmlENapiG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}